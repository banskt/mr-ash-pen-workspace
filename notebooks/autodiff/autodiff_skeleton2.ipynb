{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automatic Differentiation\n",
    "\n",
    "The `autograd` package in Python automate the computation of backward passes for **automatic differentiation**. When using `autograd`, a **computational graph** is defined under the hood. Any function written in the source code is represented as a computational graph; nodes in the graph will be tensors, and edges will be functions that produce output tensors from input tensors. Backpropagating through this graph then allows you to easily compute gradients.\n",
    "\n",
    "It is the mathematical equivalent of journeying around the world with zero planning. I can just keep composing and stacking functions, always assured that `autograd` is going to be able follow the breadcrumbs and compute a derivative for me."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Source**: [Autodidact: A tutorial implementation of Autograd](https://github.com/mattjj/autodidact)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the computational graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Tracing utilities.\n",
    "This library provides functions for constructing a computation graph. With this\n",
    "library, one can,\n",
    "- Build a computation graph. (trace)\n",
    "- Register wrapper types for unwrapped values based on type(). (Box.register)\n",
    "- Build functions that can deal with wrapped values. (primitive,\n",
    "  notrace_primitive)\n",
    "- Box values. (new_box)\n",
    "\"\"\"\n",
    "from collections import defaultdict\n",
    "from contextlib import contextmanager\n",
    "\n",
    "from .util import subvals, wraps\n",
    "\n",
    "def trace(start_node, fun, x):\n",
    "    with trace_stack.new_trace() as trace_id:\n",
    "        # Wrap 'x' in a box.\n",
    "        start_box = new_box(x, trace_id, start_node)\n",
    "\n",
    "        # Apply fun() to boxed value. This will carry the value throughout the\n",
    "        # comutation as well as the box.\n",
    "        end_box = fun(start_box)\n",
    "\n",
    "        if isbox(end_box) and end_box._trace_id == start_box._trace_id:\n",
    "            # Extract final value (== fun(x)) and its node in the computation\n",
    "            # graph.\n",
    "            return end_box._value, end_box._node\n",
    "        else:\n",
    "            # Output seems independent of input\n",
    "            return end_box, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the graph nodes. \n",
    "\n",
    "Each node has a value obtained by applying a function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node(object):\n",
    "    \"\"\"A node in a computation graph.\"\"\"\n",
    "    def __init__(self, value, fun, args, kwargs, parent_argnums, parents):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          value: output of fun(*args, **kwargs)\n",
    "          fun: wrapped numpy that was applied.\n",
    "          args: all (unboxed) positional arguments.\n",
    "          kwargs: dict of additional keyword args.\n",
    "          parent_argnums: integers corresponding to positional indices of boxed\n",
    "            values.\n",
    "          parents: Node instances corresponding to parent_argnums.\n",
    "        \"\"\"\n",
    "        self.parents = parents\n",
    "        self.recipe = (fun, value, args, kwargs, parent_argnums)\n",
    "\n",
    "    def initialize_root(self):\n",
    "        self.parents = []\n",
    "        self.recipe = (lambda x: x, None, (), {}, [])\n",
    "\n",
    "    @classmethod\n",
    "    def new_root(cls, *args, **kwargs):\n",
    "        root = cls.__new__(cls)\n",
    "        root.initialize_root(*args, **kwargs)\n",
    "        return root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def primitive(f_raw):\n",
    "    \"\"\"Wraps a function so that its gradient (vjp) can be specified and its\n",
    "    invocation can be recorded.\"\"\"\n",
    "    @wraps(f_raw)\n",
    "    def f_wrapped(*args, **kwargs):\n",
    "        # Fetch boxed arguments with largest trace_id.  This ensures that the\n",
    "        # computational graph being constructed only consists of other nodes\n",
    "        # from the same call to trace().\n",
    "        boxed_args, trace_id = find_top_boxed_args(args)\n",
    "        if boxed_args:\n",
    "            # Replace some elements of args with corresponding unboxed values.\n",
    "            argvals = subvals(args, [(argnum, box._value) for argnum, box in boxed_args])\n",
    "            # Get nodes for each boxed argument.\n",
    "            parents = tuple(box._node for _, box in boxed_args)\n",
    "\n",
    "            # Get argument indices for each boxed argument.\n",
    "            argnums = tuple(argnum for argnum, _ in boxed_args)\n",
    "\n",
    "            # Calculate result of applying original numpy function.\n",
    "            #\n",
    "            # Note that we use a recursive call here in order to also augment\n",
    "            # outer calls to trace() with lower trace_ids. See TraceStack's\n",
    "            # docstring for details.\n",
    "            ans = f_wrapped(*argvals, **kwargs)\n",
    "\n",
    "            # Create a new node\n",
    "            node = Node(ans, f_wrapped, argvals, kwargs, argnums, parents)\n",
    "            return new_box(ans, trace_id, node)\n",
    "        else:\n",
    "            return f_raw(*args, **kwargs)\n",
    "    return f_wrapped\n",
    "\n",
    "def notrace_primitive(f_raw):\n",
    "    \"\"\"Wrap a raw numpy function by discarding boxes.\n",
    "    Results are not boxed. Unboxing is a signal that the f_raw() is\n",
    "    non-differentiable with respect to its arguments. Consider the computation,\n",
    "    ```\n",
    "    x = 1.5\n",
    "    y = np.floor(x) + x\n",
    "    ```\n",
    "    What is the derivative of y wrt x? Autograd says 1. as np.floor has zero\n",
    "    derivative near x=1.5.\n",
    "    \"\"\"\n",
    "    @wraps(f_raw)\n",
    "    def f_wrapped(*args, **kwargs):\n",
    "        # Extract np.ndarray values from boxed values.\n",
    "        argvals = map(getval, args)\n",
    "\n",
    "        # Call original function. Note that f_raw()'s arguments may still be\n",
    "        # boxed, but with a lower trace_id.\n",
    "        return f_raw(*argvals, **kwargs)\n",
    "    return f_wrapped\n",
    "\n",
    "def find_top_boxed_args(args):\n",
    "    \"\"\"Finds boxed arguments with largest trace_id.\n",
    "    Equivalent to finding the largest trace_id of any argument, keeping args\n",
    "    with the same, and dropping the remainder.\n",
    "    Args:\n",
    "      args: Arguments to function wrapped by primitive().\n",
    "    Returns:\n",
    "      top_boxes: List of (index, boxed argument). Arguments have same, largest\n",
    "        trace_id.\n",
    "      top_trace_id: trace_id of all elements in top_boxes.\n",
    "    \"\"\"\n",
    "    top_trace_id = -1\n",
    "    top_boxes = []\n",
    "    for argnum, arg in enumerate(args):\n",
    "        if isbox(arg):\n",
    "            if arg._trace_id > top_trace_id:\n",
    "                top_boxes = [(argnum, arg)]\n",
    "                top_trace_id = arg._trace_id\n",
    "            elif arg._trace_id == top_trace_id:\n",
    "                top_boxes.append((argnum, arg))\n",
    "    return top_boxes, top_trace_id\n",
    "\n",
    "class TraceStack(object):\n",
    "    \"\"\"Tracks number of times trace() has been called.\n",
    "    This is critical to ensure calling grad() on a function that also calls\n",
    "    grad() resolves correctly. For example,\n",
    "    ```\n",
    "    def f(x):\n",
    "      def g(y):\n",
    "        return x * y\n",
    "      return grad(g)(x)\n",
    "    y = grad(f)(5.)\n",
    "    ```\n",
    "    First, grad(f)(5.) wraps 5. in a Box and calls f(Box(5)). Then, grad(g)(x)\n",
    "    wraps Box(5) again and calls g(Box(Box(5)). When computing grad(g), we want\n",
    "    to treat x=Box(5) as fixed -- it's not a direct argument to g(). How does\n",
    "    Autograd know that x is fixed, when all it can see is\n",
    "    np.multipy(Box(5.), Box(Box(5.))? Because the second argument has a larger\n",
    "    trace_id than the former!\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.top = -1\n",
    "\n",
    "    @contextmanager\n",
    "    def new_trace(self):\n",
    "        \"\"\"Increment trace depth.\"\"\"\n",
    "        self.top += 1\n",
    "        yield self.top\n",
    "        self.top -= 1\n",
    "\n",
    "trace_stack = TraceStack()\n",
    "\n",
    "class Box(object):\n",
    "    \"\"\"Boxes a value within a computation graph.\"\"\"\n",
    "\n",
    "    # Type -> subclasses of Box. Types may be instances of Box. Subclasses must\n",
    "    # take same arguments for __init__().\n",
    "    type_mappings = {}\n",
    "\n",
    "    # Non-Box types that can be boxed.\n",
    "    types = set()\n",
    "\n",
    "    def __init__(self, value, trace_id, node):\n",
    "        self._value = value\n",
    "        self._node = node\n",
    "        self._trace_id = trace_id\n",
    "\n",
    "    def __bool__(self):\n",
    "        return bool(self._value)\n",
    "\n",
    "    __nonzero__ = __bool__\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"Autograd {0} with value {1}\".format(\n",
    "            type(self).__name__, str(self._value))\n",
    "\n",
    "    @classmethod\n",
    "    def register(cls, value_type):\n",
    "        \"\"\"Register a class as a Box for type 'value_type'.\n",
    "        Should be called immediately after declaration.\n",
    "        Args:\n",
    "          cls: Inherits from Box. Type to box values of type 'value_type'.\n",
    "          value_type: Type to be boxed.\n",
    "        \"\"\"\n",
    "        Box.types.add(cls)\n",
    "        Box.type_mappings[value_type] = cls\n",
    "\n",
    "        # The Box implementation for a Box type is itself. Why? Imagine a nested\n",
    "        # call to grad(). One doesn't want the inner Box's computation graph to\n",
    "        # interact with the outer Box's.\n",
    "        Box.type_mappings[cls] = cls\n",
    "\n",
    "\n",
    "box_type_mappings = Box.type_mappings\n",
    "\n",
    "def new_box(value, trace_id, node):\n",
    "    \"\"\"Box an unboxed value.\n",
    "    Args:\n",
    "      value: unboxed value\n",
    "      trace_id: int. Trace stack depth.\n",
    "      node: Node corresponding to this boxed value.\n",
    "    Returns:\n",
    "      Boxed value.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return box_type_mappings[type(value)](value, trace_id, node)\n",
    "    except KeyError:\n",
    "        raise TypeError(\"Can't differentiate w.r.t. type {}\".format(type(value)))\n",
    "\n",
    "box_types = Box.types\n",
    "\n",
    "# If True, the value is Box.\n",
    "isbox  = lambda x: type(x) in box_types  # almost 3X faster than isinstance(x, Box)\n",
    "\n",
    "# Get value from a Box.\n",
    "getval = lambda x: getval(x._value) if isbox(x) else x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Graph():\n",
    "    \"\"\" Computational graph class. \n",
    "    Initilizes a global variable _g that describes the graph.\n",
    "    Each graph consists of a set of\n",
    "        1. operators\n",
    "        2. variables\n",
    "        3. constants\n",
    "        4. placeholders\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.operators = set()\n",
    "        self.constants = set()\n",
    "        self.variables = set()\n",
    "        self.placeholders = set()\n",
    "        global _g\n",
    "        _g = self\n",
    "        \n",
    "    def reset_counts(self, root):\n",
    "        if hasattr(root, 'count'):\n",
    "            root.count = 0\n",
    "        else:\n",
    "            for child in root.__subclasses__():\n",
    "                self.reset_counts(child)\n",
    "\n",
    "    def reset_session(self):\n",
    "        try:\n",
    "            del _g\n",
    "        except:\n",
    "            pass\n",
    "        self.reset_counts(Node)\n",
    "        \n",
    "    def __enter__(self):\n",
    "        return self\n",
    "    \n",
    "    def __exit__(self, exc_type, exc_value, traceback):\n",
    "        self.reset_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the graph nodes. \n",
    "\n",
    "The operator node is virtual, it is never called. Only subclasses of it containing actual operations should ever be called."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a basic Node class to inherit from\n",
    "\n",
    "This won't do anything other than allow us to check if in object is a Graph node or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Variables, Constants, Placeholders, and Operators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Placeholders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Placeholder(Node):\n",
    "    \"\"\"An placeholder node in the computational graph. This holds\n",
    "    a node, and awaits further input at computation time.\n",
    "    Args: \n",
    "        name: defaults to \"Plc/\"+count\n",
    "        dtype: the type that the node holds, float, int, etc.\n",
    "    \"\"\"\n",
    "    count = 0\n",
    "    def __init__(self, name, dtype=float):\n",
    "        _g.placeholders.add(self)\n",
    "        self.value = None\n",
    "        self.gradient = None\n",
    "        self.name = f\"Plc/{Placeholder.count}\" if name is None else name\n",
    "        Placeholder.count += 1\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return f\"Placeholder: name:{self.name}, value:{self.value}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Constant(Node):\n",
    "    \"\"\"An constant node in the computational graph.\n",
    "    Args: \n",
    "        name: defaults to \"const/\"+count\n",
    "        value: a property protected value that prevents user \n",
    "               from reassigning value\n",
    "    \"\"\"\n",
    "    count = 0\n",
    "    def __init__(self, value, name=None):\n",
    "        _g.constants.add(self)\n",
    "        self._value = value\n",
    "        self.gradient = None\n",
    "        self.name = f\"Const/{Constant.count}\" if name is None else name\n",
    "        Constant.count += 1\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return f\"Constant: name:{self.name}, value:{self.value}\"\n",
    "    \n",
    "    @property\n",
    "    def value(self):\n",
    "        return self._value\n",
    "    \n",
    "    @value.setter\n",
    "    def value(self):\n",
    "        raise ValueError(\"Cannot reassign constant\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Variable(Node):\n",
    "    \"\"\"An variable node in the computational graph. Variables are\n",
    "    automatically tracked during graph computation.\n",
    "    Args: \n",
    "        name: defaults to \"var/\"+count\n",
    "        value: a mutable value\n",
    "    \"\"\"\n",
    "    count = 0\n",
    "    def __init__(self, value, name=None):\n",
    "        _g.variables.add(self)\n",
    "        self.value = value\n",
    "        self.gradient = None\n",
    "        self.name = f\"Var/{Variable.count}\" if name is None else name\n",
    "        Variable.count += 1\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return f\"Variable: name:{self.name}, value:{self.value}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Operators\n",
    "\n",
    "This way, we can provide addition and multiplication as dunder functions, and overload the python operators '+' and '*'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Operator(Node):\n",
    "    \"\"\"An operator node in the computational graph.\n",
    "    Args: \n",
    "        name: defaults to \"operator name/\"+count\n",
    "    \"\"\"\n",
    "    def __init__(self, name='Operator'):\n",
    "        _g.operators.add(self)\n",
    "        self.value = None\n",
    "        self.inputs = []\n",
    "        self.gradient = None\n",
    "        self.name = name\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"Operator: name:{self.name}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create some actual operators that do things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class add(Operator):\n",
    "    count = 0\n",
    "    \"\"\"Binary addition operation.\"\"\"\n",
    "    def __init__(self, a, b, name=None):\n",
    "        super().__init__(name)\n",
    "        self.inputs=[a, b]\n",
    "        self.name = f'add/{add.count}' if name is None else name\n",
    "        add.count += 1\n",
    "        \n",
    "    def forward(self, a, b):\n",
    "        return a+b\n",
    "    \n",
    "    def backward(self, a, b, dout):\n",
    "        return dout, dout\n",
    "\n",
    "class multiply(Operator):\n",
    "    count = 0\n",
    "    \"\"\"Binary multiplication operation.\"\"\"\n",
    "    def __init__(self, a, b, name=None):\n",
    "        super().__init__(name)\n",
    "        self.inputs=[a, b]\n",
    "        self.name = f'mul/{multiply.count}' if name is None else name\n",
    "        multiply.count += 1\n",
    "        \n",
    "    def forward(self, a, b):\n",
    "        return a*b\n",
    "    \n",
    "    def backward(self, a, b, dout):\n",
    "        return dout*b, dout*a\n",
    "    \n",
    "class divide(Operator):\n",
    "    count = 0\n",
    "    \"\"\"Binary division operation.\"\"\"\n",
    "    def __init__(self, a, b, name=None):\n",
    "        super().__init__(name)\n",
    "        self.inputs=[a, b]\n",
    "        self.name = f'div/{divide.count}' if name is None else name\n",
    "        divide.count += 1\n",
    "   \n",
    "    def forward(self, a, b):\n",
    "        return a/b\n",
    "    \n",
    "    def backward(self, a, b, dout):\n",
    "        return dout/b, dout*a/np.power(b, 2)\n",
    "    \n",
    "    \n",
    "class power(Operator):\n",
    "    count = 0\n",
    "    \"\"\"Binary exponentiation operation.\"\"\"\n",
    "    def __init__(self, a, b, name=None):\n",
    "        super().__init__(name)\n",
    "        self.inputs=[a, b]\n",
    "        self.name = f'pow/{power.count}' if name is None else name\n",
    "        power.count += 1\n",
    "   \n",
    "    def forward(self, a, b):\n",
    "        return np.power(a, b)\n",
    "    \n",
    "    def backward(self, a, b, dout):\n",
    "        return dout*b*np.power(a, (b-1)), dout*np.log(a)*np.power(a, b)\n",
    "    \n",
    "class matmul(Operator):\n",
    "    count = 0\n",
    "    \"\"\"Binary multiplication operation.\"\"\"\n",
    "    def __init__(self, a, b, name=None):\n",
    "        super().__init__(name)\n",
    "        self.inputs=[a, b]\n",
    "        self.name = f'matmul/{matmul.count}' if name is None else name\n",
    "        matmul.count += 1\n",
    "        \n",
    "    def forward(self, a, b):\n",
    "        return a@b\n",
    "    \n",
    "    def backward(self, a, b, dout):\n",
    "        return dout@b.T, a.T@dout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For convenience, overload all of these"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def node_wrapper(func, self, other):\n",
    "    if isinstance(other, Node):\n",
    "        return func(self, other)\n",
    "    if isinstance(other, float) or isinstance(other, int):\n",
    "        return func(self, Constant(other))\n",
    "    raise TypeError(\"Incompatible types.\")\n",
    "\n",
    "Node.__add__ = lambda self, other: node_wrapper(add, self, other)\n",
    "Node.__mul__ = lambda self, other: node_wrapper(multiply, self, other)\n",
    "Node.__div__ = lambda self, other: node_wrapper(divide, self, other)\n",
    "Node.__neg__ = lambda self: node_wrapper(multiply, self, Constant(-1))\n",
    "Node.__pow__ = lambda self, other: node_wrapper(power, self, other)\n",
    "Node.__matmul__ = lambda self, other: node_wrapper(matmul, self, other)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test it out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with Graph() as g:\n",
    "    x = Variable(1.3)\n",
    "    y = Variable(0.9)\n",
    "    z = x*y+5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{Constant: name:Const/0, value:5}\n",
      "{Variable: name:Var/0, value:1.3, Variable: name:Var/1, value:0.9}\n",
      "{Operator: name:mul/0, Operator: name:add/0}\n"
     ]
    }
   ],
   "source": [
    "print(g.constants)\n",
    "print(g.variables)\n",
    "print(g.operators)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topological_sort(head_node=None, graph=_g):\n",
    "    \"\"\"Performs topological sort of all nodes prior to and \n",
    "    including the head_node. \n",
    "    Args:\n",
    "        graph: the computational graph. This is the global value by default\n",
    "        head_node: last node in the forward pass. The \"result\" of the graph.\n",
    "    Returns:\n",
    "        a sorted array of graph nodes.\n",
    "    \"\"\"\n",
    "    vis = set()\n",
    "    ordering = []\n",
    "    \n",
    "    def _dfs(node):\n",
    "        if node not in vis:\n",
    "            vis.add(node)\n",
    "            if isinstance(node, Operator):\n",
    "                for input_node in node.inputs:\n",
    "                    _dfs(input_node)\n",
    "            ordering.append(node)\n",
    "            \n",
    "    if head_node is None:\n",
    "        for node in graph.operators:\n",
    "            _dfs(node)\n",
    "    else:\n",
    "        _dfs(head_node)\n",
    "        \n",
    "    return ordering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(order, feed_dict={}):\n",
    "    \"\"\" Performs the forward pass, returning the output of the graph.\n",
    "    Args:\n",
    "        order: a topologically sorted array of nodes\n",
    "        feed_dict: a dictionary values for placeholders.\n",
    "    Returns:\n",
    "        1. the final result of the forward pass.\n",
    "        2. directly edits the graph to fill in its current values.\n",
    "    \"\"\"\n",
    "    for node in order:\n",
    "        \n",
    "        if isinstance(node, Placeholder):\n",
    "            node.value = feed_dict[node.name]\n",
    "                    \n",
    "        elif isinstance(node, Operator):\n",
    "            node.value = node.forward(*[prev_node.value for prev_node in node.inputs])\n",
    "\n",
    "    return order[-1].value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_pass(order, target_node=None):\n",
    "    \"\"\" Perform the backward pass to retrieve gradients.\n",
    "    Args:\n",
    "        order: a topologically sorted array of graph nodes.\n",
    "               by default, this assigns the graident of the final node to 1\n",
    "    Returns:\n",
    "        gradients of nodes as listed in same order as input argument\n",
    "    \"\"\"\n",
    "    vis = set()\n",
    "    order[-1].gradient = 1\n",
    "    for node in reversed(order):\n",
    "        if isinstance(node, Operator):\n",
    "            inputs = node.inputs\n",
    "            grads = node.backward(*[x.value for x in inputs], dout=node.gradient)\n",
    "            for inp, grad in zip(inputs, grads):\n",
    "                if inp not in vis:\n",
    "                    inp.gradient = grad\n",
    "                else:\n",
    "                    inp.gradient += grad\n",
    "                vis.add(inp)\n",
    "    return [node.gradient for node in order]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node ordering:\n",
      "Variable: name:x, value:0.9\n",
      "Variable: name:y, value:0.4\n",
      "Operator: name:mul/0\n",
      "Constant: name:c, value:1.3\n",
      "Operator: name:add/0\n",
      "Operator: name:mul/1\n",
      "Operator: name:add/1\n",
      "----------\n",
      "Forward pass expected: 3.0580000000000003\n",
      "Forward pass computed: 3.0580000000000003\n"
     ]
    }
   ],
   "source": [
    "val1, val2, val3 = 0.9, 0.4, 1.3\n",
    "\n",
    "with Graph() as g:\n",
    "    x = Variable(val1, name='x')\n",
    "    y = Variable(val2, name='y')\n",
    "    c = Constant(val3, name='c')\n",
    "    z = (x*y+c)*c + x\n",
    "\n",
    "    order = topological_sort(z)\n",
    "    res = forward_pass(order)\n",
    "    grads = backward_pass(order)\n",
    "\n",
    "    print(\"Node ordering:\")\n",
    "    for node in order:\n",
    "        print(node)\n",
    "\n",
    "    print('-'*10)\n",
    "    print(f\"Forward pass expected: {(val1*val2+val3)*val3+val1}\")\n",
    "    print(f\"Forward pass computed: {res}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dz/dx expected = 1.52\n",
      "dz/dx computed = 1.52\n",
      "dz/dy expected = 1.1700000000000002\n",
      "dz/dy computed = 1.1700000000000002\n",
      "dz/dc expected = 2.96\n",
      "dz/dc computed = 2.96\n"
     ]
    }
   ],
   "source": [
    "dzdx_node = [a for a in order if a.name=='x'][0]\n",
    "dzdy_node = [a for a in order if a.name=='y'][0]\n",
    "dzdc_node = [a for a in order if a.name=='c'][0]\n",
    "\n",
    "print(f\"dz/dx expected = {val3*val2+1}\")\n",
    "print(f\"dz/dx computed = {dzdx_node.gradient}\")\n",
    "\n",
    "print(f\"dz/dy expected = {val1*val3}\")\n",
    "print(f\"dz/dy computed = {dzdy_node.gradient}\")\n",
    "\n",
    "print(f\"dz/dc expected = {val1*val2+2*val3}\")\n",
    "print(f\"dz/dc computed = {dzdc_node.gradient}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
