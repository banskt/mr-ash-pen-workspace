{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automatic Differentiation\n",
    "\n",
    "The `autograd` package in Python automate the computation of backward passes for **automatic differentiation**. When using `autograd`, a **computational graph** is defined under the hood. Any function written in the source code is represented as a computational graph; nodes in the graph will be tensors, and edges will be functions that produce output tensors from input tensors. Backpropagating through this graph then allows you to easily compute gradients.\n",
    "\n",
    "It is the mathematical equivalent of journeying around the world with zero planning. I can just keep composing and stacking functions, always assured that `autograd` is going to be able follow the breadcrumbs and compute a derivative for me."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Source**: [Autodidact: A tutorial implementation of Autograd](https://github.com/mattjj/autodidact)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the computational graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Graph():\n",
    "    \"\"\" Computational graph class. \n",
    "    Initilizes a global variable _g that describes the graph.\n",
    "    Each graph consists of a set of\n",
    "        1. operators\n",
    "        2. variables\n",
    "        3. constants\n",
    "        4. placeholders\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.operators = set()\n",
    "        self.constants = set()\n",
    "        self.variables = set()\n",
    "        self.placeholders = set()\n",
    "        global _g\n",
    "        _g = self\n",
    "        \n",
    "    def reset_counts(self, root):\n",
    "        if hasattr(root, 'count'):\n",
    "            root.count = 0\n",
    "        else:\n",
    "            for child in root.__subclasses__():\n",
    "                self.reset_counts(child)\n",
    "\n",
    "    def reset_session(self):\n",
    "        try:\n",
    "            del _g\n",
    "        except:\n",
    "            pass\n",
    "        self.reset_counts(Node)\n",
    "        \n",
    "    def __enter__(self):\n",
    "        return self\n",
    "    \n",
    "    def __exit__(self, exc_type, exc_value, traceback):\n",
    "        self.reset_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the graph nodes. \n",
    "\n",
    "The operator node is virtual, it is never called. Only subclasses of it containing actual operations should ever be called."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a basic Node class to inherit from\n",
    "\n",
    "This won't do anything other than allow us to check if in object is a Graph node or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Variables, Constants, Placeholders, and Operators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Placeholders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Placeholder(Node):\n",
    "    \"\"\"An placeholder node in the computational graph. This holds\n",
    "    a node, and awaits further input at computation time.\n",
    "    Args: \n",
    "        name: defaults to \"Plc/\"+count\n",
    "        dtype: the type that the node holds, float, int, etc.\n",
    "    \"\"\"\n",
    "    count = 0\n",
    "    def __init__(self, name, dtype=float):\n",
    "        _g.placeholders.add(self)\n",
    "        self.value = None\n",
    "        self.gradient = None\n",
    "        self.name = f\"Plc/{Placeholder.count}\" if name is None else name\n",
    "        Placeholder.count += 1\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return f\"Placeholder: name:{self.name}, value:{self.value}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Constant(Node):\n",
    "    \"\"\"An constant node in the computational graph.\n",
    "    Args: \n",
    "        name: defaults to \"const/\"+count\n",
    "        value: a property protected value that prevents user \n",
    "               from reassigning value\n",
    "    \"\"\"\n",
    "    count = 0\n",
    "    def __init__(self, value, name=None):\n",
    "        _g.constants.add(self)\n",
    "        self._value = value\n",
    "        self.gradient = None\n",
    "        self.name = f\"Const/{Constant.count}\" if name is None else name\n",
    "        Constant.count += 1\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return f\"Constant: name:{self.name}, value:{self.value}\"\n",
    "    \n",
    "    @property\n",
    "    def value(self):\n",
    "        return self._value\n",
    "    \n",
    "    @value.setter\n",
    "    def value(self):\n",
    "        raise ValueError(\"Cannot reassign constant\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Variable(Node):\n",
    "    \"\"\"An variable node in the computational graph. Variables are\n",
    "    automatically tracked during graph computation.\n",
    "    Args: \n",
    "        name: defaults to \"var/\"+count\n",
    "        value: a mutable value\n",
    "    \"\"\"\n",
    "    count = 0\n",
    "    def __init__(self, value, name=None):\n",
    "        _g.variables.add(self)\n",
    "        self.value = value\n",
    "        self.gradient = None\n",
    "        self.name = f\"Var/{Variable.count}\" if name is None else name\n",
    "        Variable.count += 1\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return f\"Variable: name:{self.name}, value:{self.value}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Operators\n",
    "\n",
    "This way, we can provide addition and multiplication as dunder functions, and overload the python operators '+' and '*'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Operator(Node):\n",
    "    \"\"\"An operator node in the computational graph.\n",
    "    Args: \n",
    "        name: defaults to \"operator name/\"+count\n",
    "    \"\"\"\n",
    "    def __init__(self, name='Operator'):\n",
    "        _g.operators.add(self)\n",
    "        self.value = None\n",
    "        self.inputs = []\n",
    "        self.gradient = None\n",
    "        self.name = name\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"Operator: name:{self.name}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create some actual operators that do things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class add(Operator):\n",
    "    count = 0\n",
    "    \"\"\"Binary addition operation.\"\"\"\n",
    "    def __init__(self, a, b, name=None):\n",
    "        super().__init__(name)\n",
    "        self.inputs=[a, b]\n",
    "        self.name = f'add/{add.count}' if name is None else name\n",
    "        add.count += 1\n",
    "        \n",
    "    def forward(self, a, b):\n",
    "        return a+b\n",
    "    \n",
    "    def backward(self, a, b, dout):\n",
    "        return dout, dout\n",
    "\n",
    "class multiply(Operator):\n",
    "    count = 0\n",
    "    \"\"\"Binary multiplication operation.\"\"\"\n",
    "    def __init__(self, a, b, name=None):\n",
    "        super().__init__(name)\n",
    "        self.inputs=[a, b]\n",
    "        self.name = f'mul/{multiply.count}' if name is None else name\n",
    "        multiply.count += 1\n",
    "        \n",
    "    def forward(self, a, b):\n",
    "        return a*b\n",
    "    \n",
    "    def backward(self, a, b, dout):\n",
    "        return dout*b, dout*a\n",
    "    \n",
    "class divide(Operator):\n",
    "    count = 0\n",
    "    \"\"\"Binary division operation.\"\"\"\n",
    "    def __init__(self, a, b, name=None):\n",
    "        super().__init__(name)\n",
    "        self.inputs=[a, b]\n",
    "        self.name = f'div/{divide.count}' if name is None else name\n",
    "        divide.count += 1\n",
    "   \n",
    "    def forward(self, a, b):\n",
    "        return a/b\n",
    "    \n",
    "    def backward(self, a, b, dout):\n",
    "        return dout/b, dout*a/np.power(b, 2)\n",
    "    \n",
    "    \n",
    "class power(Operator):\n",
    "    count = 0\n",
    "    \"\"\"Binary exponentiation operation.\"\"\"\n",
    "    def __init__(self, a, b, name=None):\n",
    "        super().__init__(name)\n",
    "        self.inputs=[a, b]\n",
    "        self.name = f'pow/{power.count}' if name is None else name\n",
    "        power.count += 1\n",
    "   \n",
    "    def forward(self, a, b):\n",
    "        return np.power(a, b)\n",
    "    \n",
    "    def backward(self, a, b, dout):\n",
    "        return dout*b*np.power(a, (b-1)), dout*np.log(a)*np.power(a, b)\n",
    "    \n",
    "class matmul(Operator):\n",
    "    count = 0\n",
    "    \"\"\"Binary multiplication operation.\"\"\"\n",
    "    def __init__(self, a, b, name=None):\n",
    "        super().__init__(name)\n",
    "        self.inputs=[a, b]\n",
    "        self.name = f'matmul/{matmul.count}' if name is None else name\n",
    "        matmul.count += 1\n",
    "        \n",
    "    def forward(self, a, b):\n",
    "        return a@b\n",
    "    \n",
    "    def backward(self, a, b, dout):\n",
    "        return dout@b.T, a.T@dout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For convenience, overload all of these"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def node_wrapper(func, self, other):\n",
    "    if isinstance(other, Node):\n",
    "        return func(self, other)\n",
    "    if isinstance(other, float) or isinstance(other, int):\n",
    "        return func(self, Constant(other))\n",
    "    raise TypeError(\"Incompatible types.\")\n",
    "\n",
    "Node.__add__ = lambda self, other: node_wrapper(add, self, other)\n",
    "Node.__mul__ = lambda self, other: node_wrapper(multiply, self, other)\n",
    "Node.__div__ = lambda self, other: node_wrapper(divide, self, other)\n",
    "Node.__neg__ = lambda self: node_wrapper(multiply, self, Constant(-1))\n",
    "Node.__pow__ = lambda self, other: node_wrapper(power, self, other)\n",
    "Node.__matmul__ = lambda self, other: node_wrapper(matmul, self, other)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test it out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with Graph() as g:\n",
    "    x = Variable(1.3)\n",
    "    y = Variable(0.9)\n",
    "    z = x*y+5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{Constant: name:Const/0, value:5}\n",
      "{Variable: name:Var/0, value:1.3, Variable: name:Var/1, value:0.9}\n",
      "{Operator: name:mul/0, Operator: name:add/0}\n"
     ]
    }
   ],
   "source": [
    "print(g.constants)\n",
    "print(g.variables)\n",
    "print(g.operators)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topological_sort(head_node=None, graph=_g):\n",
    "    \"\"\"Performs topological sort of all nodes prior to and \n",
    "    including the head_node. \n",
    "    Args:\n",
    "        graph: the computational graph. This is the global value by default\n",
    "        head_node: last node in the forward pass. The \"result\" of the graph.\n",
    "    Returns:\n",
    "        a sorted array of graph nodes.\n",
    "    \"\"\"\n",
    "    vis = set()\n",
    "    ordering = []\n",
    "    \n",
    "    def _dfs(node):\n",
    "        if node not in vis:\n",
    "            vis.add(node)\n",
    "            if isinstance(node, Operator):\n",
    "                for input_node in node.inputs:\n",
    "                    _dfs(input_node)\n",
    "            ordering.append(node)\n",
    "            \n",
    "    if head_node is None:\n",
    "        for node in graph.operators:\n",
    "            _dfs(node)\n",
    "    else:\n",
    "        _dfs(head_node)\n",
    "        \n",
    "    return ordering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(order, feed_dict={}):\n",
    "    \"\"\" Performs the forward pass, returning the output of the graph.\n",
    "    Args:\n",
    "        order: a topologically sorted array of nodes\n",
    "        feed_dict: a dictionary values for placeholders.\n",
    "    Returns:\n",
    "        1. the final result of the forward pass.\n",
    "        2. directly edits the graph to fill in its current values.\n",
    "    \"\"\"\n",
    "    for node in order:\n",
    "        \n",
    "        if isinstance(node, Placeholder):\n",
    "            node.value = feed_dict[node.name]\n",
    "                    \n",
    "        elif isinstance(node, Operator):\n",
    "            node.value = node.forward(*[prev_node.value for prev_node in node.inputs])\n",
    "\n",
    "    return order[-1].value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_pass(order, target_node=None):\n",
    "    \"\"\" Perform the backward pass to retrieve gradients.\n",
    "    Args:\n",
    "        order: a topologically sorted array of graph nodes.\n",
    "               by default, this assigns the graident of the final node to 1\n",
    "    Returns:\n",
    "        gradients of nodes as listed in same order as input argument\n",
    "    \"\"\"\n",
    "    vis = set()\n",
    "    order[-1].gradient = 1\n",
    "    for node in reversed(order):\n",
    "        if isinstance(node, Operator):\n",
    "            inputs = node.inputs\n",
    "            grads = node.backward(*[x.value for x in inputs], dout=node.gradient)\n",
    "            for inp, grad in zip(inputs, grads):\n",
    "                if inp not in vis:\n",
    "                    inp.gradient = grad\n",
    "                else:\n",
    "                    inp.gradient += grad\n",
    "                vis.add(inp)\n",
    "    return [node.gradient for node in order]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node ordering:\n",
      "Variable: name:x, value:0.9\n",
      "Variable: name:y, value:0.4\n",
      "Operator: name:mul/0\n",
      "Constant: name:c, value:1.3\n",
      "Operator: name:add/0\n",
      "Operator: name:mul/1\n",
      "Operator: name:add/1\n",
      "----------\n",
      "Forward pass expected: 3.0580000000000003\n",
      "Forward pass computed: 3.0580000000000003\n"
     ]
    }
   ],
   "source": [
    "val1, val2, val3 = 0.9, 0.4, 1.3\n",
    "\n",
    "with Graph() as g:\n",
    "    x = Variable(val1, name='x')\n",
    "    y = Variable(val2, name='y')\n",
    "    c = Constant(val3, name='c')\n",
    "    z = (x*y+c)*c + x\n",
    "\n",
    "    order = topological_sort(z)\n",
    "    res = forward_pass(order)\n",
    "    grads = backward_pass(order)\n",
    "\n",
    "    print(\"Node ordering:\")\n",
    "    for node in order:\n",
    "        print(node)\n",
    "\n",
    "    print('-'*10)\n",
    "    print(f\"Forward pass expected: {(val1*val2+val3)*val3+val1}\")\n",
    "    print(f\"Forward pass computed: {res}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dz/dx expected = 1.52\n",
      "dz/dx computed = 1.52\n",
      "dz/dy expected = 1.1700000000000002\n",
      "dz/dy computed = 1.1700000000000002\n",
      "dz/dc expected = 2.96\n",
      "dz/dc computed = 2.96\n"
     ]
    }
   ],
   "source": [
    "dzdx_node = [a for a in order if a.name=='x'][0]\n",
    "dzdy_node = [a for a in order if a.name=='y'][0]\n",
    "dzdc_node = [a for a in order if a.name=='c'][0]\n",
    "\n",
    "print(f\"dz/dx expected = {val3*val2+1}\")\n",
    "print(f\"dz/dx computed = {dzdx_node.gradient}\")\n",
    "\n",
    "print(f\"dz/dy expected = {val1*val3}\")\n",
    "print(f\"dz/dy computed = {dzdy_node.gradient}\")\n",
    "\n",
    "print(f\"dz/dc expected = {val1*val2+2*val3}\")\n",
    "print(f\"dz/dc computed = {dzdc_node.gradient}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
